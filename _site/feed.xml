<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://konstantinklepikov.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://konstantinklepikov.github.io/" rel="alternate" type="text/html" /><updated>2019-10-29T01:36:48+02:00</updated><id>https://konstantinklepikov.github.io/feed.xml</id><title type="html">My deep learning</title><subtitle>Блог про нейронные сети и машинное обучение</subtitle><entry><title type="html">Подготовка данных: кодирование категориальных признаков</title><link href="https://konstantinklepikov.github.io/2019/10/28/data-preprocessing-category-encoders.html" rel="alternate" type="text/html" title="Подготовка данных: кодирование категориальных признаков" /><published>2019-10-28T00:00:00+02:00</published><updated>2019-10-28T00:00:00+02:00</updated><id>https://konstantinklepikov.github.io/2019/10/28/data-preprocessing-category-encoders</id><content type="html" xml:base="https://konstantinklepikov.github.io/2019/10/28/data-preprocessing-category-encoders.html">&lt;p&gt;В статье «&lt;a href=&quot;/2019/10/08/scikit-learn-preprocessing.html&quot;&gt;особенности препроцессинга данных в scikit-learn&lt;/a&gt;» разбирались особенности кодирования признаков с помощью библиотеки scikit-learn. К сожалению, набор инструментов scikit-learn довольно скромный.&lt;/p&gt;

&lt;p&gt;Часто данные содержат множественные категориальные признаки, часть из которых представлена несколькими категориями. В некоторых случаях категорий оказывается сравнительно много, по отношению к общему объему данных. Иногда значения в категориальных признаках распределены по-разному, в частности, могут быть определены какие-то серии, разбитые по времени. Все это не добавляет энтузиазма во время предварительной обработки данных.&lt;/p&gt;

&lt;p&gt;К счастью, есть другие готовые решения, в т.ч. библиотека &lt;strong&gt;&lt;a href=&quot;https://contrib.scikit-learn.org/categorical-encoding/&quot;&gt;Category Encoders&lt;/a&gt; (CE)&lt;/strong&gt;, предоставляющая широкий набор кодировщиков категориальных признаков.&lt;/p&gt;

&lt;h2 id=&quot;какие-преимущества-у-ce&quot;&gt;Какие преимущества у CE&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Наверное, самое основное — это полная совместимость с scikit-learn. Доступны методы fit, fit_transform, get_params, set_params и transform. На основе CE можно строить пайплайны в scikit-learn.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Поддержка numpy и pandas. Что важно — pandas dataframe можно получить и на выходе кодировщика. Иногда это весьма полезно, особенно когда нужно выполнить выборочное кодирование. Это позволяет не городить самодельный забор из кодировщиков, а использовать CE непосредственно в пайплайне scikit-learn.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Есть возможность выбрать и явно указать кодируемые столбцы&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Можно отбрасывать часть данных&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Спроектированный кодировщик отлично портируется на рабочие данные.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;какие-задачи-можно-решать-с-помощью-ce&quot;&gt;Какие задачи можно решать с помощью CE?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;кодирование номинальных признаков (nominal) — признаки, порядок которых не определен&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;кодирование упорядоченных признаков (ordinal) признаки, порядок которых не частично определен&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Можно кодировать бинарные признаки, упорядоченные по алфавиту или численному возрастанию признаки, а так же географические, геометрические данные, данные о времени и другие структурированные данные.&lt;/p&gt;

&lt;h2 id=&quot;что-имеется-в-наборе-ce&quot;&gt;Что имеется в наборе CE&lt;/h2&gt;

&lt;h3 id=&quot;contrast-coding&quot;&gt;Contrast Coding&lt;/h3&gt;

&lt;p&gt;Данный тип кодирование разбивает столбец на уровни (в каждом только значения, относящиеся к одной категории). Затем для каждого уровня вычисляется некоторая статистика. Например, вот так это делается в &lt;a href=&quot;http://www.statsmodels.org/dev/contrasts.html&quot;&gt;statsmodels&lt;/a&gt;. Метод подходит для кодирования номинальных и частично упорядоченных признаков.&lt;/p&gt;

&lt;p&gt;В CE реализованы следующие кодеры:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Backward Difference Coding — сравнивается среднее для уровня со средним предыдущего уровня&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Helmert Coding — сравнивается среднее для уровня со средним для всех последующих уровней. Больше подходит для номинальных переменных.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sum Coding — сравнивается среднее для уровня со средним для всех остальных уровней&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Polynomial Coding — используются линейные, квадратичные и кубические представления целевого признака. Подходит исключительно для упорядоченных признаков, интервалы между которыми одинаковы.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;В CE не реализованы:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Deviation Coding — более общий случай суммирующего кодирования, когда сравнение идет со всеми уровнями&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dummy Coding — сравнение со средним значением на уровне уровнем&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Simple Coding — то же самое, что и Dummy Coding, только в качестве среднего принимается среднее всех значений фиксированного уровня&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reverse Helmet Coding&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Forward Difference Coding&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Больше подробностей &lt;a href=&quot;https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/&quot;&gt;смотри тут&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;target-based-coding&quot;&gt;Target-based Coding&lt;/h3&gt;

&lt;p&gt;Для кодирования переменных используются сведения о разметке (цели) дата-сета. Для кодирования обычно используются следующие понятия: &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; общее число примеров, &lt;script type=&quot;math/tex&quot;&gt;y^+&lt;/script&gt; число примеров, размеченных «положительной» целью, &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; число примеров в уровне, &lt;script type=&quot;math/tex&quot;&gt;n^+&lt;/script&gt; число примеров уровня, отнесенных к положительному классу, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; регуляризирующий параметр, &lt;script type=&quot;math/tex&quot;&gt;prior&lt;/script&gt; среднее значение цели. В CE реализованы:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Target Encoder. Переменная кодируется по формуле &lt;script type=&quot;math/tex&quot;&gt;x^k = prior*(1 - s) + s*\frac{n^+}{n}&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;s = \frac{1}{1 + \exp(\frac{-n - mdl}{\alpha})}&lt;/script&gt;, а &lt;script type=&quot;math/tex&quot;&gt;\scriptsize mdl&lt;/script&gt; — минимум среди всех примеров на уровне.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;James-Stein Encoder кодируется по формуле &lt;script type=&quot;math/tex&quot;&gt;x^k = (1 - B) * \frac{n^+ + prior*m}{u^+ + m} + B*\frac{y^+}{y}&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; дополнительный гиперпараметр, регулирующий переобучение&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;M-estimate кодируется по формуле &lt;script type=&quot;math/tex&quot;&gt;x^k = \frac{n^+}{n} + B * \frac{y^+}{y}&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;m = 1... 100&lt;/script&gt; — дополнительный гиперпараметр, регулирующий переобучение. (На момент написания статьи кодировщик работает некорректно)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Weight of Evidence (WOE) считается по формуле &lt;script type=&quot;math/tex&quot;&gt;x^k = \ln(\frac{nominator}{denominator})&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;nominator = \frac{n^+ + \alpha}{y^+ + 2\alpha}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;denominator = \frac{n - n^+ + \alpha}{y - y^+ + 2\alpha}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Leave One Out (LOO) считается среднее цели для примера выбранной категории, для случая, когда пример удален из дата-сета.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Catboost Encoder улучшенный LOO (&lt;a href=&quot;https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html&quot;&gt;документация&lt;/a&gt;)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;У всех target-based кодировщиков имеет место проблема риска переобучения, так как используются данные о разметке дата-сета. Два варианта решения — дополнительная регуляризация и двойная кросс-валидация. Кроме того, последние два плохо работают, если реальные данные имеют другую размерность, нежели те, на которых обучалась модель.&lt;/p&gt;

&lt;h3 id=&quot;остальные-кодировщики&quot;&gt;Остальные кодировщики&lt;/h3&gt;

&lt;p&gt;В CE реализовано несколько базовых кодировщиков:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Binary аналог binarizer в scikit-learn&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One Hot аналог OnHotEncoder в scikit-learn&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Base N комбинация One Hot и Binary&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ordinal аналог LabelEncoder или OrdinalEncoder в scikit-learn&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Кроме того, реализован Hashing, позволяющий хешировать переменную. Это аналог FeatureHasher (последний больше подходит для работы с текстом).&lt;/p&gt;

&lt;p&gt;К сожалению, в библиотеке не реализован частотный кодировщик и в принципе не реализованы методы работы с временными рядами.&lt;/p&gt;

&lt;p&gt;Дополнительные статьи по этой тематике: &lt;a href=&quot;https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02&quot;&gt;один&lt;/a&gt;, &lt;a href=&quot;https://towardsdatascience.com/benchmarking-categorical-encoders-9c322bd77ee8&quot;&gt;два&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="preprocessing" /><category term="category-encoders" /><category term="sklearn" /><category term="scikit-learn" /><category term="ml-data" /><summary type="html">В статье «особенности препроцессинга данных в scikit-learn» разбирались особенности кодирования признаков с помощью библиотеки scikit-learn. К сожалению, набор инструментов scikit-learn довольно скромный.</summary></entry><entry><title type="html">Вычислительная сложность машинного обучения. Базовые принципы</title><link href="https://konstantinklepikov.github.io/2019/10/19/complexity-basics-terms.html" rel="alternate" type="text/html" title="Вычислительная сложность машинного обучения. Базовые принципы" /><published>2019-10-19T00:00:00+02:00</published><updated>2019-10-19T00:00:00+02:00</updated><id>https://konstantinklepikov.github.io/2019/10/19/complexity-basics-terms</id><content type="html" xml:base="https://konstantinklepikov.github.io/2019/10/19/complexity-basics-terms.html">&lt;p&gt;Чуть ранее, в статье &lt;a href=&quot;/2019/09/08/time-complexity-of-machine-learning-algorithms.html&quot;&gt;временная сложность алгоритмов машинного обучения&lt;/a&gt;, я разбирал временную сложность некоторых алгоритмов из библиотеки scykit-learn. Настало время немного подробнее остановиться на том, как в принципе считается вычислительная сложность data science.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Вычислительная сложность&lt;/strong&gt; (или асимптотическая сложность или производительность) — это свойство алгоритма. Она определяется функцией, которая показывает насколько ухудшается работа алгоритма с усложнением поставленной задачи.&lt;/p&gt;

&lt;p&gt;Вот пять основных правил расчета вычислительной сложности:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;если для математической функции &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; алгоритму необходимо выполнить действия &lt;script type=&quot;math/tex&quot;&gt;f(N)&lt;/script&gt; раз, то для этого ему понадобится сделать &lt;script type=&quot;math/tex&quot;&gt;O(f(N))&lt;/script&gt; шагов.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;если алгоритм выполняет одну операцию, состоящую из &lt;script type=&quot;math/tex&quot;&gt;O(f(N))&lt;/script&gt; шагов, а затем вторую, состоящую из &lt;script type=&quot;math/tex&quot;&gt;O(g(N))&lt;/script&gt; шагов, то общая производительность f и g суммируется &lt;script type=&quot;math/tex&quot;&gt;O(f(N) + g(N))&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;если алгоритму необходимо сделать &lt;script type=&quot;math/tex&quot;&gt;O(f(N) + g(N))&lt;/script&gt; шагов и область значений N функции &lt;script type=&quot;math/tex&quot;&gt;f(N)&lt;/script&gt; больше чем у &lt;script type=&quot;math/tex&quot;&gt;g(N)&lt;/script&gt;, то вычислительную сложность можно упростить до &lt;script type=&quot;math/tex&quot;&gt;f(N)&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;если алгоритму внутри каждого шага &lt;script type=&quot;math/tex&quot;&gt;O(f(N))&lt;/script&gt; одной операции приходится выполнять еще &lt;script type=&quot;math/tex&quot;&gt;O(g(N))&lt;/script&gt; шагов другой операции, то общая производительность составляет &lt;script type=&quot;math/tex&quot;&gt;O(f(N)*g(N))&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;постоянными множителями (константами) можно пренебречь: &lt;script type=&quot;math/tex&quot;&gt;O(C*f(N))&lt;/script&gt; и &lt;script type=&quot;math/tex&quot;&gt;O(f(C*N))&lt;/script&gt; можно записать как &lt;script type=&quot;math/tex&quot;&gt;O(f(N))&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Нотация &lt;strong&gt;«O» большое&lt;/strong&gt; всего лишь указывает на то, что мы рассматриваем ситуацию, когда алгоритму для завершения необходимо потребить  максимальное возможное количество (худший случай). В предыдущей статье рассматривался частный случай, входящий в обобщенное понятие вычислительной сложности — временная сложность алгоритмов МО. Грубо говоря, объем задачи алгоритма всегда связан с вычислительными ресурсами — временем (или количеством шагов) вычислений и пространством (или объемом памяти), необходимыми для завершения задачи. Поэтому в различной специальной литературе по машинному обучению, где рассматриваются частные случаи алгоритмов, можно встретить отсылки как к вычислительной сложности в целом так и к сложности по времени или по памятми.&lt;/p&gt;

&lt;p&gt;На практике чаще всего встречаются следующие сложности:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;O(1)&lt;/script&gt; вне зависимости от сложности задачи время выполнения алгоритма постоянно&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;O(\log(N))&lt;/script&gt; на каждом шаге алгоритма происходит деление количества рассматриваемых элементов на фиксированный коэффициент&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;O(N)&lt;/script&gt; рост числа входов вызывает линейный рост производительности&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;O(N*\log(N))&lt;/script&gt; алгоритм с логарифмической сложностью, на каждом шаге которого производится дополнительная операция с каждым элементом&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;O(N^2)&lt;/script&gt; перебор всех данных, а затем повторный их перебор. Степень может быть другой, что, очевидно, влияет на сложность вычислений.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;O(2^N)&lt;/script&gt; экспоненциальная сложность и &lt;script type=&quot;math/tex&quot;&gt;O(!N)&lt;/script&gt; факториальная сложность&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Задачи со сложностью до &lt;script type=&quot;math/tex&quot;&gt;O(N*\log(N))&lt;/script&gt; включительно решаемы. При грамотном управлении количеством входных данных решаемы и задачи со степенной сложностью. Экспоненциальные и факториальные сложности, в виду в целом большого входного объема данных, в МО неприменимы.&lt;/p&gt;

&lt;p&gt;Ключевым вопросом в оценке сложности алгоритмов МО является их класс, определяющий требования ко времени и ресурсам памяти, применительно к некой абстрактной машине (часто рассматривается детерминированная машина, в частности, машина Тьюринга). Для детерминированных машин определены следующие классы:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;DTIME(f(N))&lt;/script&gt; задачи, которые машина решает за время &lt;script type=&quot;math/tex&quot;&gt;f(N)&lt;/script&gt;. Временная сложность таким образом будет составлять &lt;script type=&quot;math/tex&quot;&gt;O(f(N))&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; задачи, с которыми машина справляется за полиномиальное время&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;EXPTIME(EXP)&lt;/script&gt; задачи, с которыми машина справляется за экспоненциальное время&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Для недетерминированных машин:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;NTIME(f(N))&lt;/script&gt; задачи, которые машина решает за время &lt;script type=&quot;math/tex&quot;&gt;f(N)&lt;/script&gt;. Временная сложность, таким образом будет, составлять &lt;script type=&quot;math/tex&quot;&gt;O(f(N))&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;NP&lt;/script&gt; задачи, с которыми машина справляется за полиномиальное время&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;NEXPTIME(EXP)&lt;/script&gt; задачи, с которыми машина справляется за экспоненциальное время&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Аналогичным образом задачи делятся в зависимости от потребляемого объема памяти.&lt;/p&gt;

&lt;p&gt;Не вдаваясь в подроности, в общем случае &lt;script type=&quot;math/tex&quot;&gt;P \subseteq NP&lt;/script&gt;, экспоненциальные задачи, по сути, не считаемые, а задачей построения алгоритмов МО, в том числе, является их сведение к менее затратному классу. Задачи, которые входят в класс NP и к которым можно свести любые другие задачи этого класса за полиномиальное время называются NP-полными. NP-сложные задачи необязательно относятся к классу NP.&lt;/p&gt;

&lt;h2 id=&quot;время-обучения-для-алгоритмов-мо&quot;&gt;Время обучения для алгоритмов МО&lt;/h2&gt;

&lt;p&gt;Фактическое время работы алгоритма МО зависит от конкретной машины, на которой алгоритм реализован. В общем случае алгоритм обучения с учителем имеет доступ к множеству примеров, классу гипотез, функции потерь и обучающему набору, взятому из множества примеров. Четкого понятия размера входных данных для такого алгоритма не существует, т.к. если мы предъявляем алгоритму избыточное количество обучающих примеров, он может игнорировать лишние. Поэтому увеличение размера обучающего набора не ведет к тому, что проблема обучения становится более трудной. кроме того, алгоритм обучения может передавать часть вычислений выходной гипотезе, в случае, когда такая гипотеза определена как функция, сохраняющая обучающий набор. Для этого вводятся понятия времени обучения и времени предсказания. Будем рассматривать алгоритмы, чье время предсказания не превышает время обучения.&lt;/p&gt;

&lt;p&gt;Вычислительная сложность алгоритма обучения определяется в два этапа:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;дана функция &lt;script type=&quot;math/tex&quot;&gt;f : (0, 1)^2 \rightarrow \mathbb{N}&lt;/script&gt;, задача обучения &lt;script type=&quot;math/tex&quot;&gt;(Z, H, l)&lt;/script&gt; и алгоритм обучения &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;. Алгоритм &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; решает задачу обучения за время &lt;script type=&quot;math/tex&quot;&gt;O(f)&lt;/script&gt;, если существует постоянное число &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; такое, что для любого распределения вероятности &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; на &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; и входных параметров &lt;script type=&quot;math/tex&quot;&gt;\epsilon, \delta \in (0, 1)&lt;/script&gt; справедливо следующее утверждение: если &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; имеет доступ к примерам, независимо выбранным из распределения &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;, то, во-первых, &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; завершается, выполнив не более &lt;script type=&quot;math/tex&quot;&gt;c f(\epsilon, \delta)&lt;/script&gt; операций, во-вторых, выход &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; обозначаемый как &lt;script type=&quot;math/tex&quot;&gt;h{\scriptstyle A}&lt;/script&gt; можно применять для предсказания метки нового примера и при этом будет выполнено не более &lt;script type=&quot;math/tex&quot;&gt;c f(\epsilon, \delta)&lt;/script&gt; операций и, в-третьих, выход &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; вероятно почти корректен, т.е. с вероятностью не ниже &lt;script type=&quot;math/tex&quot;&gt;1 - \delta&lt;/script&gt; (для случайной выборки, которую получает &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;) &lt;script type=&quot;math/tex&quot;&gt;{L_{\scriptscriptstyle D}}({h_{\scriptscriptstyle A}}) \leq {\min_{\scriptscriptstyle h'\in H}} {L_{\scriptscriptstyle D}}{(h'_{\scriptscriptstyle A}}) + \epsilon&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;далее, рассматриваем последовательность проблем обучения &lt;script type=&quot;math/tex&quot;&gt;{({Z_{\scriptscriptstyle n}}, {H_{\scriptscriptstyle n}}, {l_{\scriptscriptstyle n}})^{\scriptscriptstyle{\infty}}_{\scriptscriptstyle n=1}}&lt;/script&gt;, где проблема &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; описывается областью примеров &lt;script type=&quot;math/tex&quot;&gt;{Z_{\scriptscriptstyle n}}&lt;/script&gt;, классом гипотез &lt;script type=&quot;math/tex&quot;&gt;{H_{\scriptscriptstyle n}}&lt;/script&gt; и функцией потерь &lt;script type=&quot;math/tex&quot;&gt;{l_{\scriptscriptstyle n}}&lt;/script&gt;. Если &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; спроектирован для проблем обучения такого вида и задана функция &lt;script type=&quot;math/tex&quot;&gt;g : \mathbb{N} x (0, 1)^2 \rightarrow \mathbb{N}&lt;/script&gt;, то считается, что время работы &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; на вышеупомянутой последовательности равно &lt;script type=&quot;math/tex&quot;&gt;O(g)&lt;/script&gt;, если для всех &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; решает проблему &lt;script type=&quot;math/tex&quot;&gt;({Z_{\scriptscriptstyle n}}, {H_{\scriptscriptstyle n}}, {l_{\scriptscriptstyle n}})&lt;/script&gt; за время  &lt;script type=&quot;math/tex&quot;&gt;O({f_{\scriptscriptstyle n}})&lt;/script&gt;, где функция &lt;script type=&quot;math/tex&quot;&gt;f : (0, 1)^2 \rightarrow \mathbb{N}&lt;/script&gt; определяется выражением &lt;script type=&quot;math/tex&quot;&gt;{f_{\scriptscriptstyle n}}(\epsilon, \delta) = {g_{\scriptscriptstyle n}}(n, \epsilon, \delta)&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;В таком случае можно сказать, что алгоритм &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; эффективен на последовательности &lt;script type=&quot;math/tex&quot;&gt;({Z_{\scriptscriptstyle n}}, {H_{\scriptscriptstyle n}}, {l_{\scriptscriptstyle n}})&lt;/script&gt;, если его время работы равно $$O(p(n, 1/\epsilon, 1/\delta)) для некоторого полинома p. Очевидно, что вопрос об эффективном решении проблемы обучения зависит от того, как именно задача обучения представлена в виде последовательности конкретных проблем.&lt;/p&gt;

&lt;p&gt;Более подробную информацию на эту тему можно найти в учебнике «Understanding Machine Learning», изданном в Cambridge University Press в 2014-ом году.&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><category term="algorithms" /><category term="time-complexity" /><summary type="html">Чуть ранее, в статье временная сложность алгоритмов машинного обучения, я разбирал временную сложность некоторых алгоритмов из библиотеки scykit-learn. Настало время немного подробнее остановиться на том, как в принципе считается вычислительная сложность data science.</summary></entry><entry><title type="html">Особенности препроцессинга данных в scikit-learn</title><link href="https://konstantinklepikov.github.io/2019/10/08/scikit-learn-preprocessing.html" rel="alternate" type="text/html" title="Особенности препроцессинга данных в scikit-learn" /><published>2019-10-08T00:00:00+02:00</published><updated>2019-10-08T00:00:00+02:00</updated><id>https://konstantinklepikov.github.io/2019/10/08/scikit-learn-preprocessing</id><content type="html" xml:base="https://konstantinklepikov.github.io/2019/10/08/scikit-learn-preprocessing.html">&lt;p&gt;В статье кратко раскрываются некоторые вопросы подготовки данных с помощью scikit-learn.&lt;/p&gt;

&lt;h3 id=&quot;замена-пропусков&quot;&gt;Замена пропусков&lt;/h3&gt;

&lt;p&gt;Scikit-learn не поддерживает замену пропусков с разными значениями. Сначала придется последовательно перегнать все интересующие пропуски, к примеру, в &lt;em&gt;NaN&lt;/em&gt;, а затем использовать инструменты препроцессинга.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html&quot;&gt;MissingIndicator&lt;/a&gt; позволяет сделать разметку пропусков. С помощью &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html&quot;&gt;SimpleImputer&lt;/a&gt; можно выполнить замену. Поддерживаются четыре основных метода:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mean&lt;/li&gt;
  &lt;li&gt;most_frequent&lt;/li&gt;
  &lt;li&gt;median&lt;/li&gt;
  &lt;li&gt;constant (необходимо задать fill_value, чтобы не получить дефолтное значение)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Препроцессинг перегоняет данные в Numpy-формат, что означает потерю метаданных. Если это представляет проблему - препроцессинг делать средствами Pandas.&lt;/p&gt;

&lt;p&gt;Есть еще несколько популярных методов, например, кластеризация с использованием K ближайших соседей и интерполяция. Оба метода не поставляются в scikit-learn. придется поискать реализации поверх.&lt;/p&gt;

&lt;h3 id=&quot;полиномиальные-признаки&quot;&gt;Полиномиальные признаки&lt;/h3&gt;

&lt;p&gt;Перед работой с &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html&quot;&gt;PolynomialFeatures&lt;/a&gt; надо иметь представление о том, как будут обрабатываться пропуски, так как NaN поднимет ошибку, а 0 для полиномов всех степеней останется нулем.&lt;/p&gt;

&lt;p&gt;Есть возможность создавать матрицу без степенных вариаций (для этого необходимо задать interaction_only=True).&lt;/p&gt;

&lt;p&gt;Весь процесс очень затратен по оперативной памяти и на нем довольно легко столкнуться нехваткой, поэтому, если дата-сет большой, придется предварительно подумать как с ним работать.&lt;/p&gt;

&lt;h3 id=&quot;категориальные-признаки&quot;&gt;Категориальные признаки&lt;/h3&gt;

&lt;p&gt;Scikit-learn не поддерживает обработку категориальных признаков, только замену на численное представление. Вариантов два:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html&quot;&gt;OrdinalEncoder&lt;/a&gt; для численного представления без разделения на отдельные признаки (пропуски тоже кодируются в собственный отдельный класс)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html&quot;&gt;OneHotEncoder&lt;/a&gt; для численного представления с разделением на отдельные признаки с бинарными классами для каждого признака.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;обработка-численных-признаков&quot;&gt;Обработка численных признаков&lt;/h3&gt;

&lt;p&gt;Численные признаки могут можно дискретизировать с помощью scikit-learn и, таким образом, перегонять их в категориальные. Поддерживаются два основных способа:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;discretisation (или квантилизация или биннинг). Доступно в виде &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html&quot;&gt;KBinsDiscretizer&lt;/a&gt; с тремя методами: uniform (одинаковая длина бинов), quantile (одинаковое число точек в бинах), kmeans (значение определяется кластеризацией).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;binarisation с помощью &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.binarize.html&quot;&gt;binarize&lt;/a&gt; — задается trashold, все что ниже или рано 0, все что выше 1.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;препроцессинг&quot;&gt;Препроцессинг&lt;/h2&gt;

&lt;p&gt;Собственно непосредственно сам препроцессинг описан в &lt;a href=&quot;https://scikit-learn.org/stable/modules/preprocessing.html&quot;&gt;разделе Preprocessing data&lt;/a&gt;. В scikit-learn это трансформация и нормализация данных. Делать это необходимо, так как многие алгоритмы чувствительны к выбросам, а так же распределению данных в выборке.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html&quot;&gt;StandardScaler&lt;/a&gt; центрирует данные, удаляет среднее значение для каждого объекта, а затем масштабирует, деля на среднее отклонение. &lt;script type=&quot;math/tex&quot;&gt;x{\scriptstyle scaled} = \frac{x - u}{s}&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; среднее, а &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; отклонение.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html&quot;&gt;MinMaxScaler&lt;/a&gt; трансформирует признаки в выбранном диапазоне. &lt;script type=&quot;math/tex&quot;&gt;x{\scriptstyle scaled} = \frac{x - \min(x)}{\max(x) - \min(x)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html&quot;&gt;MaxAbsScaler&lt;/a&gt; трансформирует в диапазон &lt;script type=&quot;math/tex&quot;&gt;[-1, 1]&lt;/script&gt;. Используется для центрированных вокруг нуля или разреженных данных. &lt;script type=&quot;math/tex&quot;&gt;x{\scriptstyle scaled} = \frac{x}{\max(abs(x))}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html&quot;&gt;RobustScaler&lt;/a&gt;. Для данных, в которых много выбросов.&lt;/p&gt;

&lt;p&gt;Для нормализации данных можно использовать &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html&quot;&gt;Normalizer&lt;/a&gt;. Довольно часто это становится необходимым, когда алгоритм предсказывает, базируясь на взвешенных значениях, основанных на расстояниях между точками данных. Особенно актуально для классификации текста и кластеризации. В scikit-learn доступны три регуляризатора:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;l1&lt;/li&gt;
  &lt;li&gt;l2&lt;/li&gt;
  &lt;li&gt;MaxAbsScaler&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="machine-learning" /><category term="scikit-learn" /><category term="ml-data" /><category term="preprocessing" /><summary type="html">В статье кратко раскрываются некоторые вопросы подготовки данных с помощью scikit-learn.</summary></entry><entry><title type="html">Зависимость вычислений в scikit-learn от данных и модели</title><link href="https://konstantinklepikov.github.io/2019/09/28/computation-performance-of-scikit-learn-functions.html" rel="alternate" type="text/html" title="Зависимость вычислений в scikit-learn от данных и модели" /><published>2019-09-28T00:00:00+02:00</published><updated>2019-09-28T00:00:00+02:00</updated><id>https://konstantinklepikov.github.io/2019/09/28/computation-performance-of-scikit-learn-functions</id><content type="html" xml:base="https://konstantinklepikov.github.io/2019/09/28/computation-performance-of-scikit-learn-functions.html">&lt;h2 id=&quot;производительность-вычислений&quot;&gt;Производительность вычислений&lt;/h2&gt;

&lt;p&gt;В scikit-learn производительность вычисления предсказаний зависит от:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;количества фичей&lt;/li&gt;
  &lt;li&gt;распределения и разреженности данных&lt;/li&gt;
  &lt;li&gt;временной сложности алгоритма&lt;/li&gt;
  &lt;li&gt;извлечения фичей&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;количество-фичей&quot;&gt;Количество фичей&lt;/h3&gt;

&lt;p&gt;Библиотека хорошо оптимизирована под небольшие дата-сеты, поэтому количество фичей начинает оказывать значительное влияние на время предсказания для данных с 250+ фичами.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../assets/img/280919-01.jpg&quot; alt=&quot;Влияние количества фичей на время предсказания&quot; title=&quot;Влияние количества фичей на время предсказания&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;распределение-и-разреженность-данных&quot;&gt;Распределение и разреженность данных&lt;/h3&gt;

&lt;p&gt;Вычисления по разреженным дата-сетам (более 90% значений) можно улучшить по времени с помощью ScyPy, т.к. библиотека оптимизирована по потреблению кеша CPU. Подробнее о том, как готовить разреженные данные, можно посмотреть в &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;документации ScyPy&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;временная-сложность-алгоритмов&quot;&gt;Временная сложность алгоритмов&lt;/h3&gt;

&lt;p&gt;С ростом временной сложности падает производительность вычислений алгоритма на предсказаниях. В примерах - зависимость задержки вычисления предсказания от временной сложности линейных моделей, SVM и ансамблей.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../assets/img/280919-02.jpg&quot; alt=&quot;Зависимость задержки вычислений предсказаний от временной сложности для линейных моделей&quot; title=&quot;Зависимость от временной сложности для линейных моделей&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../assets/img/280919-03.jpg&quot; alt=&quot;Зависимость задержки вычислений предсказаний от временной сложности для SVM&quot; title=&quot;Зависимость от временной сложности для SVM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../assets/img/280919-04.jpg&quot; alt=&quot;Зависимость задержки вычислений предсказаний от временной сложности для ансамблей&quot; title=&quot;Зависимость от временной сложности для ансамблей&quot; /&gt;&lt;/p&gt;

&lt;p&gt;В линейных моделях используется схожая решающая функция, поэтому время предсказаний для разных линейных моделей зависит примерно одинаково от временной сложности модели. В алгоритмах с нелинейными кернелами вычислительная производительность зависит от количества векторов (чем больше, тем производительность меньше). Задержка вычисления предсказания возрастает линейно для SVM (для регрессора и классификатора) с ростом числа саппортных векторов. Для ансамблей наибольшее значение имеет количество решающих деревьев и их глубина.&lt;/p&gt;

&lt;p&gt;Про временную сложность построения алгоритмов и подробнее про сложность расчета предсказаний можно почитать в &lt;a href=&quot;/2019/09/08/time-complexity-of-machine-learning-algorithms.html&quot;&gt;отдельной статье&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;извлечение-фичей&quot;&gt;Извлечение фичей&lt;/h3&gt;

&lt;p&gt;На самом деле, препроцессинг данных занимает самую значительную часть времени в выдаче предсказаний. Часто очистка и трансформация данных может увеличивать задержку выдачи предсказаний в сотни раз, поэтому этот процесс необходимо постоянно внимательно и аккуратно улучшать.&lt;/p&gt;

&lt;h2 id=&quot;пропускная-способность-предсказаний&quot;&gt;Пропускная способность предсказаний&lt;/h2&gt;

&lt;p&gt;Еще одна важная метрика производительности алгоритма - это пропускная способность, определяющая количество предсказаний за заданное время. В примере пропускная способность различных алгоритмов scikit-learn.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../assets/img/280919-05.jpg&quot; alt=&quot;Пропускная способность различных алгоритмов scikit-learn&quot; title=&quot;Пропускная способность различных алгоритмов scikit-learn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Основной способ повышения пропускной способности предсказывающей модели - это увеличение количества экземпляров модели и распределение запросов на предсказания между экземплярами.&lt;/p&gt;

&lt;p&gt;Больше информации и примеров про производительность вычислений scikit-learn можно прочитать в &lt;a href=&quot;https://scikit-learn.org/stable/documentation.html&quot;&gt;технической документации&lt;/a&gt; (смотрите &lt;a href=&quot;https://scikit-learn.org/stable/modules/computing.html&quot;&gt;раздел 7&lt;/a&gt; user guide).&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><category term="algorithms" /><category term="time-complexity" /><category term="scikit-learn" /><category term="ml-data" /><category term="computation-performance" /><summary type="html">Производительность вычислений</summary></entry><entry><title type="html">Основные термины машинного обучения. Часть №2</title><link href="https://konstantinklepikov.github.io/2019/09/14/machine_learning-terms-part-two.html" rel="alternate" type="text/html" title="Основные термины машинного обучения. Часть №2" /><published>2019-09-14T00:00:00+02:00</published><updated>2019-09-14T00:00:00+02:00</updated><id>https://konstantinklepikov.github.io/2019/09/14/machine_learning-terms-part-two</id><content type="html" xml:base="https://konstantinklepikov.github.io/2019/09/14/machine_learning-terms-part-two.html">&lt;p&gt;В &lt;a href=&quot;/2019/08/31/machine_learning-terms.html&quot;&gt;первой части&lt;/a&gt; статьи я разобрал базовую терминологию ML: постановку задачи, емкость, переобучение и недообучение, регуляризацию и гиперпараметры, точечную оценку, смещение оценки, дисперсию, стандартную ошибку и состоятельность, а так-же важные термины, определяемые в рамках этих терминов. Продолжим.&lt;/p&gt;

&lt;h2 id=&quot;оценка-максимального-правдоподобия&quot;&gt;Оценка максимального правдоподобия&lt;/h2&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta{\tiny ML} = \underset{\theta}{\arg\max}\ p{\tiny model}(\mathbb{X};\theta) = \underset{\theta}{\arg\max}\ \underset{i=1}{\overset{m}{\prod}}\ p{\tiny model}(\mathbf{x}^{(i)};\theta)&lt;/script&gt;, где:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbb{X} = \{ x^{(1)}, ... x^{(m)} \}&lt;/script&gt; — множество, состоящее из &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; примеров, независимо выбираемых из неизвестного порождающего распределения &lt;script type=&quot;math/tex&quot;&gt;p{\tiny data}(\mathbf{x})&lt;/script&gt;. В выражении максимального правдоподобия &lt;script type=&quot;math/tex&quot;&gt;p{\tiny model}(\mathbf{x};\theta)&lt;/script&gt; — параметрическое семейство распределений вероятности над  одним и тем же пространством, индексированное параметром &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Произведение неудобно по причине, т.к. подвержено потере значимости. Взятие логарифма не изменяет &lt;script type=&quot;math/tex&quot;&gt;\arg\max&lt;/script&gt;, но позволяет преобразовать произведение в сумму: &lt;script type=&quot;math/tex&quot;&gt;\theta{\tiny ML} = \underset{\theta}{\arg\max}\ \underset{i=1}{\overset{m}{\sum}}\ \log\ p{\tiny model}(\mathbf{x}^{(i)};\theta)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Если разделить правую часть на &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; (умножение функции стоимости на константу не изменяет &lt;script type=&quot;math/tex&quot;&gt;\arg\max&lt;/script&gt;), мы получаем математическое ожидание относительного эмпирического распределения &lt;script type=&quot;math/tex&quot;&gt;\hat{p}{\tiny data}&lt;/script&gt; определяемого обучающими данными: &lt;script type=&quot;math/tex&quot;&gt;\theta{\tiny ML} = \underset{\theta}{\arg\max}\ \mathbb{E}{\scriptsize x-\hat{p}{\tiny data}}\ \log\ p{\tiny model}(\mathbf{x};\theta)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Максимальное правдоподобие — это попытка совместить модельное распределение с эмпирическим &lt;script type=&quot;math/tex&quot;&gt;\hat{p}{\tiny data}&lt;/script&gt;, в идеале мы хотим получить совпадение истинного и порождающего распределения &lt;script type=&quot;math/tex&quot;&gt;p{\tiny data}&lt;/script&gt;. Это интерпретируется с помощью минимизации расхождения Кульбака-Лейблера.&lt;/p&gt;

&lt;h3 id=&quot;условное-логарифмическое-правдоподобие&quot;&gt;Условное логарифмическое правдоподобие&lt;/h3&gt;

&lt;p&gt;Если &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; представляет все входы, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{Y}&lt;/script&gt; все наблюдаемые выходы, а все примеры независимы и одинаково распределены, то условное логарифмическое правдоподобие: &lt;script type=&quot;math/tex&quot;&gt;\theta{\tiny ML} = \underset{\theta}{\arg\max}\ \underset{i=1}{\overset{m}{\sum}}\ \log\ P(\mathbf{y}^{(i)}\mid\mathbf{x}^{(i)};\theta)&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;байесовская-статистика&quot;&gt;Байесовская статистика&lt;/h2&gt;

&lt;p&gt;В отличие от частотного метода, в котором предполагается, что истинное значение &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; фиксировано хотя и неизвестно, а точечная оценка &lt;script type=&quot;math/tex&quot;&gt;\hat{\theta}&lt;/script&gt; — случайная величина, в байесовском подходе к статистике истинный параметр &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; неизвестен или недостоверен и представляется случайной величиной, а набор данных случайной величиной не является, т.к. доступен прямому наблюдению. До наблюдения данных  мы представляем свое знание о &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; в качестве априорного распределения вероятности &lt;script type=&quot;math/tex&quot;&gt;p(\theta)&lt;/script&gt;. Тогда можно реконструировать влияние данных на наши гипотезы о &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, объединив правдоподобие данных с априорным посредством теоремы Байеса:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;p(\theta\mid x^{(1)}, ... x^{(m)}) = \frac{p(x^{(1)}, ... x^{(m)}\mid\theta)p(\theta)}{p(x^{(1)}, ... x^{(m)})}&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;x^{(1)}, ... x^{(m)})&lt;/script&gt; — набор наблюдаемых примеров.&lt;/p&gt;

&lt;p&gt;В отличие от оценки максимального правдоподобия, где предсказания делаются с использованием точечной оценки &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, в байесовской оценке  предсказания делаются с помощью полного распределения &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. к примеру, после наблюдения &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; примеров предсказанное распределение следующего &lt;script type=&quot;math/tex&quot;&gt;x^{m+1}&lt;/script&gt; примера описывается формулой: &lt;script type=&quot;math/tex&quot;&gt;p(x^{m+1}\mid x^{(1)}, ... x^{(m)}) = \int p(x^{m+1}\mid\theta)p(\theta\mid x^{(1)}, ... x^{(m)})d\theta&lt;/script&gt;. Если после наблюдения &lt;script type=&quot;math/tex&quot;&gt;x^{(1)}, ... x^{(m)}&lt;/script&gt; примеров мы все еще не знаем &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, то эта неопределенность включается непосредственно в предсказания.&lt;/p&gt;

&lt;p&gt;Кроме того, при байесовской оценке происходит сдвиг плотности вероятности в сторону тех областей пространства параметров, которые априори предпочтительны, что обусловлено значительным влиянием байесовского априорного распределения. Зачастую это приводит к предпочтению более простых и гладких моделей.&lt;/p&gt;

&lt;p&gt;Байесовские модели обобщаются лучше при ограниченном числе обучающих данных, но с ростом данных обучение становится вычислительно более накладным.&lt;/p&gt;

&lt;h3 id=&quot;оценка-априорного-максимума&quot;&gt;Оценка априорного максимума&lt;/h3&gt;

&lt;p&gt;В большинстве случаев операции, включающие апостериорное байесовское распределение, недопустимы с точки зрения временной сложности алгоритмов. В этом случае точечная оценка &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; дает разрешимую апроксимацию. Чтобы использовать преимущества байесовской оценки, разрешив априорному распределению влиять на выбор точечной оценки, применяют оценку апостериорного максимум (MAP):&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta{\tiny MAP} = \underset{\theta}{\arg\max}\ p(\theta\mid\mathbf{x})\ = \underset{\theta}{\arg\max}\ \log\ p(\theta\mid\mathbf{x}) + \log\ p(\theta)&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;\log\ p(\theta\mid\mathbf{x})&lt;/script&gt; — стандартное логарифмическое правдоподобие, а &lt;script type=&quot;math/tex&quot;&gt;\log\ p(\theta)&lt;/script&gt; соответствует априорному распределению.&lt;/p&gt;

&lt;h2 id=&quot;проблемы-требующие-глубокого-изучения&quot;&gt;Проблемы, требующие глубокого изучения&lt;/h2&gt;

&lt;h3 id=&quot;проклятие-размерности&quot;&gt;Проклятие размерности&lt;/h3&gt;

&lt;p&gt;С увеличением размерности данных количество представляющих интерес конфигураций растет экспоненциально. Если имеется &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; измерений и нужно различать &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; значений вдоль каждой оси, то потребуется &lt;script type=&quot;math/tex&quot;&gt;O(v^{d})&lt;/script&gt; областей и примеров.&lt;/p&gt;

&lt;h3 id=&quot;регуляризация-для-достижения-локального-постоянства-и-гладкости&quot;&gt;Регуляризация для достижения локального постоянства и гладкости&lt;/h3&gt;

&lt;p&gt;Чтобы алгоритм хорошо обобщался, необходимо иметь априорное представление о том, какого рода функцию он должен обучить. Самое распространенные априорные предположения — &lt;strong&gt;априорное предположение о гладкости&lt;/strong&gt; или &lt;strong&gt;априорное предположение о локальном постоянстве&lt;/strong&gt;. Это означает, что обучаемая функция не должна сильно изменяться в небольшой области.&lt;/p&gt;

&lt;p&gt;Обобщаемость большинства алгоритмов опирается на этот принцип, поэтому они плохо масштабируются на многие статистические задачи.&lt;/p&gt;

&lt;h3 id=&quot;обучение-многообразий&quot;&gt;Обучение многообразий&lt;/h3&gt;

&lt;p&gt;В основе ML лежит концепция &lt;strong&gt;многообразия&lt;/strong&gt; — множества точек, ассоциированных с окрестностью каждой точки. Из этой концепции вытекает существование преобразований для перемещения из одного места многообразия в другое.&lt;/p&gt;

&lt;p&gt;В ML многообразие — это связное множество точек в пространстве высокой размерности, которое можно хорошо аппроксимировать, вводя в рассмотрение лишь небольшое число степеней свободы, или измерений. В машинном обучении допускаются многообразия, размерность которых различна в разных точках.&lt;/p&gt;

&lt;p&gt;Многие алгоритмы ML безнадежны, если ожидается, что в результате обучения алгоритм должен найти функции с нетривиальными изменениями во всем пространстве &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{n}&lt;/script&gt;. Алгоритмы обучения многообразий преодолевают это препятствие, предполагая, что большая часть &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{n}&lt;/script&gt; — недопустимые входные данные, а интересующие нас входы сосредоточены только в наборе многообразий, содержащем небольшое подмножество точек, причем интересные изменения результирующей функции будут происходить только вдоль направлений, принадлежащих какому-то одному многообразию, или при переходе с одного многообразия на другое.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Данное краткое описание составлено на основе книги «Глубокое обучение» за авторством Я.Гудфеллоу, И.Бенджио, А.Курвилль&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><summary type="html">В первой части статьи я разобрал базовую терминологию ML: постановку задачи, емкость, переобучение и недообучение, регуляризацию и гиперпараметры, точечную оценку, смещение оценки, дисперсию, стандартную ошибку и состоятельность, а так-же важные термины, определяемые в рамках этих терминов. Продолжим.</summary></entry><entry><title type="html">Временная сложность алгоритмов машинного обучения</title><link href="https://konstantinklepikov.github.io/2019/09/08/time-complexity-of-machine-learning-algorithms.html" rel="alternate" type="text/html" title="Временная сложность алгоритмов машинного обучения" /><published>2019-09-08T00:00:00+02:00</published><updated>2019-09-08T00:00:00+02:00</updated><id>https://konstantinklepikov.github.io/2019/09/08/time-complexity-of-machine-learning-algorithms</id><content type="html" xml:base="https://konstantinklepikov.github.io/2019/09/08/time-complexity-of-machine-learning-algorithms.html">&lt;p&gt;&lt;strong&gt;Временная сложность&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;T(n)&lt;/script&gt; определяет количество операций, которые необходимо выполнить алгоритму для обработки входных данных объемом n.&lt;/p&gt;

&lt;p&gt;Показатель сложности усредняется, но на практике необходимо исходить из худшего случая, при котором для обработки входных данных требуется максимальное количество операций. Для этого используется так называемая нотация &lt;strong&gt;«O» большое&lt;/strong&gt;. Грубо говоря, &lt;script type=&quot;math/tex&quot;&gt;O(n)&lt;/script&gt; выражает доминантный член функции стоимости алгоритма в худшем случае.&lt;/p&gt;

&lt;p&gt;Пример из &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_complexity&quot;&gt;статьи в википедии&lt;/a&gt;, демонстрирующий рост числа операций с ростом объема входных данных для алгоритмов разной временной сложности.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../assets/img/080919-1.jpg&quot; alt=&quot;time complexity&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Далее в этой статье я попробую собрать данные о временной сложности различных реализаций ML/DL алгоритмов. В статье &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; — количество признаков, &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; — количество образцов.&lt;/p&gt;

&lt;h2 id=&quot;алгоритмы-из-scikit-learn&quot;&gt;Алгоритмы из scikit-learn&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors&quot;&gt;Nearest Neighbors Algorithms&lt;/a&gt;&lt;/strong&gt;. Временная сложность зависит от имплементации.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;brute force (наиболее нативная имплементация, дистанция считается между всеми точками дата-сета). Сложность при построении &lt;script type=&quot;math/tex&quot;&gt;O(n*m^{2})&lt;/script&gt;. Сложность при запросе &lt;script type=&quot;math/tex&quot;&gt;O(n*m)&lt;/script&gt;. Время расчета не зависит от структуры данных и количества «ближайших соседей».&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;K-D tree. Сложность при построении &lt;script type=&quot;math/tex&quot;&gt;O(n*m\log(m))&lt;/script&gt;. Сложность при запросе для небольшого числа фич &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
n &lt; 20 %]]&gt;&lt;/script&gt; — &lt;script type=&quot;math/tex&quot;&gt;O(n\log(m))&lt;/script&gt;. Для большего числа измерений, сложность возрастает до &lt;script type=&quot;math/tex&quot;&gt;O(n*m)&lt;/script&gt;. Время расчета сильно зависит от структуры данных и растет с увеличением количества «ближайших соседей».&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ball Tree. Сложность при построении &lt;script type=&quot;math/tex&quot;&gt;O(n*m\log(m))&lt;/script&gt;. Сложность при запросе — &lt;script type=&quot;math/tex&quot;&gt;O(n\log(m))&lt;/script&gt;. Время расчета сильно зависит от структуры данных и растет с увеличением количества «ближайших соседей».&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Больше подробностей смотри в &lt;a href=&quot;https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms&quot;&gt;документации&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html&quot;&gt;LinearRegression&lt;/a&gt;&lt;/strong&gt;. Временная сложность &lt;script type=&quot;math/tex&quot;&gt;O(m*n^{2,4})&lt;/script&gt; — &lt;script type=&quot;math/tex&quot;&gt;O(m*n^{3})&lt;/script&gt; в зависимости от реализации.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html&quot;&gt;SGDClassifier&lt;/a&gt;&lt;/strong&gt;. Временная сложность &lt;script type=&quot;math/tex&quot;&gt;O(m*n)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html&quot;&gt;SVC&lt;/a&gt;&lt;/strong&gt;. Временная сложность &lt;script type=&quot;math/tex&quot;&gt;O(m^{2}*n)&lt;/script&gt; — &lt;script type=&quot;math/tex&quot;&gt;O(m^{3}*n)&lt;/script&gt;. Алгоритм реализован на библиотеке &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html&quot;&gt;LinearSVC&lt;/a&gt;&lt;/strong&gt;. Временная сложность &lt;script type=&quot;math/tex&quot;&gt;O(m*n)&lt;/script&gt;. Алгоритм реализован на базе библиотеки liblinear, который реализует &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;Large Linear Classification&lt;/a&gt; (подробнее &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf&quot;&gt;тут&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/tree.html&quot;&gt;DecisionTree&lt;/a&gt;&lt;/strong&gt;. Нахождение оптимального дерева является &lt;a href=&quot;https://en.wikipedia.org/wiki/NP-completeness&quot;&gt;NP-полной задачей&lt;/a&gt; и ее временная сложность &lt;script type=&quot;math/tex&quot;&gt;O(\exp(m))&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Временная сложность построения сбалансированного бинарного дерева для наивной имплементации &lt;script type=&quot;math/tex&quot;&gt;O(n*m\log(m))&lt;/script&gt;, сложность расчета по построенной модели — &lt;script type=&quot;math/tex&quot;&gt;O(\log(m))&lt;/script&gt;. На практике сбалансированного дерева не получается, поэтому сложность построения дерева возростает до &lt;script type=&quot;math/tex&quot;&gt;O(n*m^{2}\log(m))&lt;/script&gt;. В scikit-learn используется препроцессинг, что позволяет уменьшить итоговую сложность для всего дерева до &lt;script type=&quot;math/tex&quot;&gt;O(n*m\log(m))&lt;/script&gt;. &lt;a href=&quot;https://scikit-learn.org/stable/modules/tree.html#complexity&quot;&gt;Подробнее&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html&quot;&gt;DecisionTreeClassifier&lt;/a&gt;. Обход дерева имеет временную сложность &lt;script type=&quot;math/tex&quot;&gt;O(\frac{\log(m)}{\log(2)})&lt;/script&gt;, ну или &lt;script type=&quot;math/tex&quot;&gt;O(\log{\scriptscriptstyle 2}(m))&lt;/script&gt;. Если осуществляется сравнение по всем признакам, то &lt;script type=&quot;math/tex&quot;&gt;O(n*m\log(m))&lt;/script&gt;, поэтому важно определять количество сравниваемых признаков.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/ensemble.html#parameters&quot;&gt;RandomForest&lt;/a&gt;&lt;/strong&gt;. &lt;script type=&quot;math/tex&quot;&gt;O(M*m\log(m))&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; — число решающих деревьев. Сложность можно уменьшить с помощью параметров.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neural_network&quot;&gt;Multi-layer Perceptron&lt;/a&gt;&lt;/strong&gt;. &lt;script type=&quot;math/tex&quot;&gt;O(m*n*h^{k}*o*i)&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt; — количество нейронов, &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; — число скрытых слоев, &lt;script type=&quot;math/tex&quot;&gt;o&lt;/script&gt; — количество выходов и &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; — число итераций.&lt;/p&gt;

&lt;h3 id=&quot;алгоритмы-кластеризации&quot;&gt;Алгоритмы кластеризации&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html&quot;&gt;Affinity Propogation&lt;/a&gt;&lt;/strong&gt;. &lt;script type=&quot;math/tex&quot;&gt;O(n^{2}*t)&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; — количество примеров, &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; — число итераций до сходимости. Сложность по памяти при этом &lt;script type=&quot;math/tex&quot;&gt;O(n^{2})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html&quot;&gt;DBSCAN&lt;/a&gt;&lt;/strong&gt;. &lt;script type=&quot;math/tex&quot;&gt;O(n*d)&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; — количество примеров, &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; — среднее число соседей. Сложность по памяти при этом линейная &lt;script type=&quot;math/tex&quot;&gt;O(n)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html&quot;&gt;OPTICS&lt;/a&gt;&lt;/strong&gt;. &lt;script type=&quot;math/tex&quot;&gt;O(n^2)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html&quot;&gt;kMean&lt;/a&gt;&lt;/strong&gt;. Средняя сложность по времени &lt;script type=&quot;math/tex&quot;&gt;O(k*n*t)&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; — количество примеров, &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; — число итераций до сходимости, &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; число соседей. В худшем случае сложность вырастает до &lt;script type=&quot;math/tex&quot;&gt;O(n^{k + \frac{2}{p}})&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; число фичей.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html&quot;&gt;MeanShift&lt;/a&gt;&lt;/strong&gt;. При использовании flat или ball tree кернелов, сложность по времени &lt;script type=&quot;math/tex&quot;&gt;O(t*n*\log(n))&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; — количество точек кластеризации. В многомерном пространстве сложность стремится к &lt;script type=&quot;math/tex&quot;&gt;O(t*n^2)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Статья будет дополняться…&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><category term="algorithms" /><category term="time-complexity" /><category term="scikit-learn" /><summary type="html">Временная сложность определяет количество операций, которые необходимо выполнить алгоритму для обработки входных данных объемом n.</summary></entry><entry><title type="html">Основные термины машинного обучения. Часть №1</title><link href="https://konstantinklepikov.github.io/2019/08/31/machine_learning-terms.html" rel="alternate" type="text/html" title="Основные термины машинного обучения. Часть №1" /><published>2019-08-31T00:00:00+02:00</published><updated>2019-08-31T00:00:00+02:00</updated><id>https://konstantinklepikov.github.io/2019/08/31/machine_learning-terms</id><content type="html" xml:base="https://konstantinklepikov.github.io/2019/08/31/machine_learning-terms.html">&lt;p&gt;Практически все алгоритмы машинного обучения можно описать как комбинацию набора данных, функции стоимости, процедуры оптимизации и модели. Любой из этих компонентов можно заменить, как правило, независмо от других. Такая формула построения алгоритма обучения подходит для обучения как с учителем, так и без. Пройдем по терминам, которые позволяют определить machine learning.&lt;/p&gt;

&lt;p&gt;Алгоритм обучается на опыте &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt; относительно некоего класса задач &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; и меры качества &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt;, если качество на задачах из &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;, измеряемое с помощью &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt;, возрастает с ростом опыта &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;задача-t&quot;&gt;Задача &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;&lt;/h3&gt;

&lt;p&gt;Есть несколько типов задач, которые можно решить с помощью машинного обучения.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Классификация&lt;/strong&gt;. В задачах этого типа алгоритм должен ответить, какой из &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; категорий принадлежит некоторый пример. Для решения этой задачи алгоритм обучения обычно просят породить функцию &lt;script type=&quot;math/tex&quot;&gt;f : \mathbb{R}^{n} \rightarrow \{1, ..., l\}&lt;/script&gt;. Если &lt;script type=&quot;math/tex&quot;&gt;y = f(\mathbf{x})&lt;/script&gt;, то модель относит входной пример, описываемый вектором &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt;, к категории с числовым кодом &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;. Есть и другие варианты классификации, например, когда &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; — распределение вероятности принадлежности к классам.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Классификация при отсутствии части данных&lt;/strong&gt;. Если часть входных данных отсутствует, алгоритм должен обучить набор функций, вместо единственной, отображающей входной вектор на код категории. Один из способов обучить такое подмножество функций — обучить распределение вероятности всех релевантных величин, а затем решить задачу классификации, вычислив маргинальное распределение отсутствующих значений.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Регрессия&lt;/strong&gt;. Алгоритм должен предсказать числовое значение по входным данным. Для решения этой задачи необходимо породить функцию &lt;script type=&quot;math/tex&quot;&gt;f : \mathbb{R}^{n} \rightarrow \mathbb{R}&lt;/script&gt;. Результатом регрессии является прогноз некоего значения.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Транскрипция&lt;/strong&gt;. В задаче этого типа предлагается проанализировать неструктурированное представление данных и преобразовать его в текст.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Машинный перевод&lt;/strong&gt;. Входные данные — это последовательность данных на одном языке, а алгоритм должен преобразовать ее в последовательность символов на другом языке.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Структурный вывод&lt;/strong&gt;.  В этой задаче на выходе порождается вектор (или иная структура, содержащая несколько значений), между элеменатами которого существуют некие, имеющие значение, связи. В сущности, в эту задачу входят и транскрипция с машинным переводом, а также грамматический разбор.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Обнаружение аномалий&lt;/strong&gt;. В данной задаче алгоритм анализирует входные данные и размечает часть из них, как аномальные.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Синтез и выборка&lt;/strong&gt;. В данной задаче алгоритм генерирует новые данные, сходные с обучающими данными. Примером может служить создание текстур или образов для компьютерных игр.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Подстановка отсутствующих значений&lt;/strong&gt;. Алгоритму предъявляется новый пример &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} \in \mathbb{R}^{n}&lt;/script&gt;, в котором некоторые элементы &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; отсутствуют. Алгоритм должен спрогнозировать значение отсутствующих элементов.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Шумоподавление&lt;/strong&gt;. В этой задаче алгоритму предъявляется не искаженный помехами пример &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\tilde{x}} \in \mathbb{R}^{n}&lt;/script&gt;, полученный из чистого примера &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} \in \mathbb{R}^{n}&lt;/script&gt;, в результате неизвестного процесса искажения. Алгоритм должен восстановить чистый пример &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; по искаженному &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\tilde{x}}&lt;/script&gt; либо вернуть &lt;script type=&quot;math/tex&quot;&gt;P(\mathbf{x}\vert\mathbf{\tilde{x}})&lt;/script&gt; условное распределение вероятности.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Оценка функции вероятности или плотности функции вероятности&lt;/strong&gt;. В данной задаче алгоритм должен обучить функцию &lt;script type=&quot;math/tex&quot;&gt;P{\tiny model} : \mathbb{R}^{n} \rightarrow \mathbb{R}&lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;P{\tiny model}(\mathbf{x})&lt;/script&gt; интерпретируется как функция плотности вероятности (если &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; непрерывная случайная величина) или как функция вероятности (дискретная величина), в пространстве, из которого были взяты примеры. Для решения этой задачи алгоритм должен уметь оценивать структуру данных, хотя бы неявно улавливать структуру распределения вероятности, а в задаче оценки плотности — явно.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;мера-качества-p&quot;&gt;Мера качества &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt;&lt;/h3&gt;

&lt;p&gt;Мера качества специфична для каждого алгоритма. Для задач классификации в основном измеряется accuracy (точность) модели — доля примеров, для которых модель выдала верное предсказание. Частота ошибок — противоположный вариант меры качества, показывающая долю примеров, для которых модель выдала неверное предсказание.&lt;/p&gt;

&lt;h3 id=&quot;опыт-e&quot;&gt;Опыт &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt;&lt;/h3&gt;

&lt;p&gt;Алгоритмы ML делятся на два больших класса:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Алгоритму обучения без учителя&lt;/strong&gt; предоставляются наборы данных, содержащих множество признаков, алгоритм должен выявить полезные структурные признаки набора.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Алгоритму обучения с учителем&lt;/strong&gt; предъявляются наборы данных, примеры в которых снабжены меткой (целевым классом)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Обучение без учителя означает наблюдение нескольких примеров случайного вектора &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; с последующей попыткой вывести, явно или неявно, распределение вероятности &lt;script type=&quot;math/tex&quot;&gt;P(\mathbf{x})&lt;/script&gt; или некие свойства этого распределения. Обучение с учителем сводится к наблюдению нескольких примеров случайного вектора &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; и ассоциированию с ним значения или вектора &lt;script type=&quot;math/tex&quot;&gt;\mathbf{y}&lt;/script&gt; с последующей попыткой вывести оценку &lt;script type=&quot;math/tex&quot;&gt;P(\mathbf{x}\vert\mathbf{y})&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Обучение с учителем и без — понятия довольно размытые, многие алгоритмы подходят для решения и той и другой задачи. Принято считать, что задачи классификации, регрессии и структурного вывода относятся к обучению с учителем, а задача оценки плотности — к обучению без учителя.&lt;/p&gt;

&lt;p&gt;Возможны и другие парадигмы обучения: обучение с частичным привлечением учителя, обучение с подкреплением и т.д.&lt;/p&gt;

&lt;h3 id=&quot;ёмкость-переобучение-и-недообучение&quot;&gt;Ёмкость, переобучение и недообучение&lt;/h3&gt;

&lt;p&gt;Способность алгоритма ML хорошо работать на новых данных, которые он ранее не видел, называется &lt;strong&gt;обобщением&lt;/strong&gt;. Мера ошибки алгоритма на обучающем наборе данных называется &lt;strong&gt;ошибкой обучения&lt;/strong&gt; — ее необходимо минимизировать (задача оптимизации). &lt;strong&gt;Ошибкой обобщения&lt;/strong&gt; называют математическое ожидание ошибки алгоритма на новых входных данных. Считается справедливым предположение, что данные из обучающего и тестового набора одинаково распределены, т.е. выбраны из одного и того же распределения вероятности, которое называется &lt;strong&gt;порождающим распределением&lt;/strong&gt;. В этом контексте &lt;strong&gt;недообучение&lt;/strong&gt; имеет место, когда модель не позволяет получить достаточно малую ошибку на обучающем наборе, а &lt;strong&gt;переобучение&lt;/strong&gt; — когда разрыв между ошибками обучения и тестирования слишком велик. &lt;strong&gt;Емкость (capacity)&lt;/strong&gt; позволяет управлять склонностью модели к переобучению или недообучению. Емкость описывает способность модели к аппроксимации широкого спектра функций. При маленькой емкости модель слишком простая и недообучается, при высокой — слишком сложная и переобучается.&lt;/p&gt;

&lt;p&gt;Один из способов контроля за емкостью — выбор &lt;strong&gt;пространства гипотез&lt;/strong&gt;, множества функций, которые алгоритм может рассматривать в качестве потенциального решения. &lt;strong&gt;Репрезентативная емкость&lt;/strong&gt; определяет семейство функций, из которой модель может выбрать алгоритм обучения в процессе варьирования параметров. Как правило, по причине дополнительных ограничений, например, из-за несовершенства оптимизации, эффективная емкость алгоритма оказывается меньше репрезентативной.&lt;/p&gt;

&lt;h3 id=&quot;теорема-об-отсутствии-бесплатных-завтраков&quot;&gt;Теорема об отсутствии бесплатных завтраков&lt;/h3&gt;

&lt;p&gt;В среднем, по всем возможным порождающим определениям у любого алгоритма классификации частота ошибок классификации ранее не наблюдавшихся примеров одинакова. Это означает, что самый сложный алгоритм, в среднем (по всем возможным задачам) дает такое же качество, как и простейший. Цель ML заключается не в том, чтобы построить самый сложный или самый эффективный алгоритм, а в том, чтобы понять, какие виды распределений характерны реальным данным.&lt;/p&gt;

&lt;h3 id=&quot;регуляризация&quot;&gt;Регуляризация&lt;/h3&gt;

&lt;p&gt;Регуляризация — это любая модификация алгоритма обучения, предпринятая с целью уменьшить его ошибку обобщения, не уменьшив при этом ошибку обучения. Из теоремы «об отсутствии бесплатных завтраках», в том числе вытекает то, что не существует наилучшего способа регуляризации.&lt;/p&gt;

&lt;h3 id=&quot;гиперпараметры&quot;&gt;Гиперпараметры&lt;/h3&gt;

&lt;p&gt;Гиперпараметры управляют поведением алгоритма ML, при этом сам алгоритм не ищет значений гиперпараметров.&lt;/p&gt;

&lt;p&gt;Попытка обучить гиперпараметр на обучающем наборе приводит к максимизации емкости модели и переобучению. Чтобы решить эту проблему, используется &lt;strong&gt;контрольный набор&lt;/strong&gt;, который формируется из обучающего и никогда не используется в обучении. Если данных слишком мало, разделение на обучающий и тестовый наборы становится проблематичным, так как приводит к статистической недостоверности в оценке средней ошибки. Эту проблему решает &lt;strong&gt;перекрестная проверка&lt;/strong&gt; — разделение исходного набора данных на подмножества и случайный выбор обучающего и контрольного набора в процессе обучения.&lt;/p&gt;

&lt;h3 id=&quot;точечная-оценка&quot;&gt;Точечная оценка&lt;/h3&gt;

&lt;p&gt;Точечная оценка — это попытка найти единственное «наилучшее» представление интересующей величины. Это может быть один или несколько параметров либо некая функция.&lt;/p&gt;

&lt;p&gt;Если &lt;script type=&quot;math/tex&quot;&gt;\tilde{\theta}&lt;/script&gt; — оценка параметра, а &lt;script type=&quot;math/tex&quot;&gt;\{\mathbf{x}^{(1)}, ..., \mathbf{x}^{(m)}\}&lt;/script&gt; — множество &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; независимых т одинаково распределенных точек, то &lt;strong&gt;точечной оценкой&lt;/strong&gt; или &lt;strong&gt;статистикой&lt;/strong&gt; называется любая оценка этих данных: &lt;script type=&quot;math/tex&quot;&gt;\tilde{\theta} = g(\mathbf{x}^{(1)}, ..., \mathbf{x}^{(m)})&lt;/script&gt;. В этом определении не требуется, чтобы &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; возвращала значение, близкое к истинному значению &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; или даже чтобы область значений &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; совпадала со множеством допустимых значений &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. При этом хорошей оценкой будет та, которая близка к истинному распределению &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, из которого выбирались обучающие данные.&lt;/p&gt;

&lt;p&gt;Точечную оценку также можно рассматривать как оценку связи между входной или выходной величинами. такой тип точечных оценок называется &lt;strong&gt;оценкой функций&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;смещение-оценки&quot;&gt;Смещение оценки&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;bias(\tilde{\theta}{\tiny m}) = \mathbb{E}(\tilde{\theta}{\tiny m}) - \theta&lt;/script&gt;, где:&lt;/p&gt;

&lt;p&gt;математическое ожидание вычисляется по данным (рассматриваемым как выборка из случайной величины), &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; — истинное значение параметра, которое определяет порождающее определение. &lt;script type=&quot;math/tex&quot;&gt;\tilde{\theta}&lt;/script&gt; является &lt;strong&gt;несмещаной&lt;/strong&gt;, если &lt;script type=&quot;math/tex&quot;&gt;bias(\tilde{\theta}{\tiny m}) = 0&lt;/script&gt;. Напротив, &lt;script type=&quot;math/tex&quot;&gt;\tilde{\theta}&lt;/script&gt; &lt;strong&gt;асимптотически смещена&lt;/strong&gt;, если &lt;script type=&quot;math/tex&quot;&gt;\lim{\scriptscriptstyle m\rightarrow\infty}\mathbb{E}(\tilde{\theta}{\tiny m}) = 0&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;дисперсия-и-стандартная-ошибка&quot;&gt;Дисперсия и стандартная ошибка&lt;/h3&gt;

&lt;p&gt;Дисперсия оценки измеряет, как будет изменяться оценка, вычисленная по данным, при независимой повторной выборке из набора данных, генерируемого порождающим процессом. Желательны оценки, обладающие не только маленьким смещением, но и маленькой дисперсией.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Var(\tilde{\theta})&lt;/script&gt;, где случайной величиной является обучающий набор.&lt;/p&gt;

&lt;p&gt;Стандартная ошибка — это квадратный корень из дисперсии. &lt;script type=&quot;math/tex&quot;&gt;SE(\tilde{\theta})&lt;/script&gt;. На практике часто используется &lt;strong&gt;стандартная ошибка среднего&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;SE(\tilde{\mu}{\scriptscriptstyle m})&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;состоятельность&quot;&gt;Состоятельность&lt;/h3&gt;

&lt;p&gt;Схождение точечной оценки к истинным значениям при увеличении числа примеров называется состоятельностью: &lt;script type=&quot;math/tex&quot;&gt;\lim{\scriptscriptstyle m\rightarrow\infty}\tilde{\theta}{\scriptscriptstyle m} = 0&lt;/script&gt;. Состоятельность гарантирует, что смещение оценки уменьшается с ростом числа примеров. При этом обратное неверно.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Данное краткое описание составлено на основе книги «Глубокое обучение» за авторством Я.Гудфеллоу, И.Бенджио, А.Курвилль&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><summary type="html">Практически все алгоритмы машинного обучения можно описать как комбинацию набора данных, функции стоимости, процедуры оптимизации и модели. Любой из этих компонентов можно заменить, как правило, независмо от других. Такая формула построения алгоритма обучения подходит для обучения как с учителем, так и без. Пройдем по терминам, которые позволяют определить machine learning.</summary></entry><entry><title type="html">API scikit-learn</title><link href="https://konstantinklepikov.github.io/2019/08/27/sklearn-processing.html" rel="alternate" type="text/html" title="API scikit-learn" /><published>2019-08-27T00:00:00+02:00</published><updated>2019-08-27T00:00:00+02:00</updated><id>https://konstantinklepikov.github.io/2019/08/27/sklearn-processing</id><content type="html" xml:base="https://konstantinklepikov.github.io/2019/08/27/sklearn-processing.html">&lt;p&gt;API scikit-learn включает три основных интерфейса:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;estimator для построения и обучения моделей&lt;/li&gt;
  &lt;li&gt;predictor для рассчета предсказаний&lt;/li&gt;
  &lt;li&gt;transformer для предварительной обработки данных&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;estimators&quot;&gt;Estimators&lt;/h3&gt;

&lt;p&gt;Этот интерфейс является базовым для библиотеки. Все алгоритмы (включая обучение без учителя) scikit-learn используют этот интерфейс. Манипуляции с данными осуществляются с помощью метода fit, а все параметры задаются в виде строк.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;penalty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;l1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Estimators используются не только для классических алгоритмов обучения (с учителем или без), но и для препроцессинга данных или извлечения важности признаков.&lt;/p&gt;

&lt;h3 id=&quot;predictors&quot;&gt;Predictors&lt;/h3&gt;

&lt;p&gt;Данный интерфейс добавляет метод predict, который позволяет получить предсказания для алгоритмов обучения с учителем. Метод стандартно возвращает предсказанные метки для классификации или значения для регрессии.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;transformers&quot;&gt;Transformers&lt;/h3&gt;

&lt;p&gt;Интерфейс создан для предварительной обработки данных, для чего используется метод transform. Метод доступен для препроцессинга, отбора и извлечения признаков и сокращения размерности.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Необходимо вызвать метод fit(), затем метод transform(). Естественно, можно в одну строку.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;дополнительные-возможности-api&quot;&gt;Дополнительные возможности API&lt;/h2&gt;

&lt;h3 id=&quot;meta-estimators&quot;&gt;Meta-estimators&lt;/h3&gt;

&lt;p&gt;Часть алгоритмов (например, ансамбли) параметризуются с помощью более простых алгоритмов. В scikit-learn часть алгоритмов имплементирована как meta-estimators. К примеру, можно сделать обертку для логистической регрессии, заменив, таким образом, дефолтный алгоритм OneVsRest.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.multiclass&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OneVsOneClassifier&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ovo_lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OneVsOneClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;penalty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;l1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;pipelines-and-feature-union&quot;&gt;Pipelines and feature union&lt;/h3&gt;

&lt;p&gt;API scikit-learn позволяет собирать собственный estimator из нескольких базовых. Это можно сделать с помощью Pipeline объекта или FeatureUnion. Pipeline объединяет разные оценочные функции в одну в виде последовательно выполняемой цепи функций. FeatureUnion объединяет функции таким образом, что их выходы конкатенируются. Оба объекта могут использоваться совместно.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.pipeline&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FeatureUnion&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.decomposition&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PCA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KernelPCA&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.featute_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SelectKBest&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;union&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FeatureUnion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pca&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PCA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()),&lt;/span&gt;
                      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;kpca&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KernelPCA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rbf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;feat_union&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;union&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;feat_scl&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SelectLBest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;log-reg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisitcRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;penalty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;l2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;model-selection&quot;&gt;Model selection&lt;/h3&gt;

&lt;p&gt;Scikit-learn упрощает поиск оптимальных значений в пространстве гиперпараметров. Для этого используются две мета-функции: GridSearcCV и RandomizedSearchCV. Так же опциональными являются кросс-валидация и скор-функции, такие как AUC и среднеквадратичная ошибка. Пример поиска гиперпараметров для SVM-классификатора:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.gridsearch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GridSearchCV&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.svm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;param_grid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;kernel&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;linear&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;C”&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;kernel&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rbf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;C&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;gamma&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GridSearchCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;f1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Больше подробностей в &lt;a href=&quot;https://scikit-learn.org/&quot;&gt;документации&lt;/a&gt; scikit-learn.&lt;/p&gt;</content><author><name></name></author><category term="scikit-learn" /><category term="phyton" /><category term="sklearn" /><category term="API" /><summary type="html">API scikit-learn включает три основных интерфейса:</summary></entry><entry><title type="html">Метрики оценки для отбора моделей в scikit-learn</title><link href="https://konstantinklepikov.github.io/2019/08/09/sklearn-scoring-argument.html" rel="alternate" type="text/html" title="Метрики оценки для отбора моделей в scikit-learn" /><published>2019-08-09T00:00:00+02:00</published><updated>2019-08-09T00:00:00+02:00</updated><id>https://konstantinklepikov.github.io/2019/08/09/sklearn-scoring-argument</id><content type="html" xml:base="https://konstantinklepikov.github.io/2019/08/09/sklearn-scoring-argument.html">&lt;p&gt;В scikit-learn можно воспользоваться встроенными метриками для отбора модели. Это можно сделать с помощью аргумента scoring, который доступен в том числе и для кросс-валидации и грид-серча. Список доступен в модуле sklearn.metrics.scorer&lt;/p&gt;

&lt;p&gt;Выглядит это примерно так:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics.scorer&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SCORERS&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SCORERS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'adjusted_rand_score'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'average_precision'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'f1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'f1_macro'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'f1_micro'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'f1_samples'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'f1_weighted'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'log_loss'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mean_absolute_error'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'mean_squared_error'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'median_absolute_error'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'precision'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'precision_macro'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'precision_micro'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'precision_samples'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'precision_weighted'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'recall'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'recall_macro'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'recall_micro'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'recall_samples'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'recall_weighted'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'roc_auc'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;В sklearn.metrics можно получить confusion_matrix. Это позволяет построить матрицу ошибок как для бинарной, так и для мультиклассовой классификации.&lt;/p&gt;

&lt;p&gt;Для классификации лучше всего построены:&lt;/p&gt;

&lt;p&gt;accuracy — дефолтная метрика для классификации&lt;/p&gt;

&lt;p&gt;f1, f1_macro, f1_micro, f1_weighted определяют f-measure (гаромническое среднее precision и recall). f1 для бинарной классификации. Macro вычисляет f-measure для каждого класса и находит их взвешенное среднее, в независимости от размера класса. Micro считает общее количество falls positive, falls negative и true positvie для каждого примера по всем классам, а затем считает f-measure. Weighted - дефолтная метрика, считается f-measure для каждого класса, находит взвешенное с учетом количества данных в каждом классе.&lt;/p&gt;

&lt;p&gt;precision = &lt;script type=&quot;math/tex&quot;&gt;\frac{TP}{TP+FP}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;recall =  &lt;script type=&quot;math/tex&quot;&gt;\frac{TP}{TP+FN}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;f = 2*&lt;script type=&quot;math/tex&quot;&gt;\frac{recall*precision}{recall+precision}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;roc_auc — площадь под ROC-кривой&lt;/p&gt;

&lt;p&gt;average_precision — площадь под кривой precision/recall&lt;/p&gt;

&lt;p&gt;Для регрессий имеют значение mean_squared_error — среднеквадратическая ошибка и median_absolute_error — средняя абсолютная ошибка.&lt;/p&gt;

&lt;p&gt;Описание доступно на &lt;a href=&quot;https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules&quot;&gt;странице документации&lt;/a&gt;. Там же можно найти подробную документацию о применении скоринговых функций для классификации и регрессии, методы применения нескольких метрик, а так же описание make_scorer ддя создания кастомных скоринговых функций с помощью make_scorer.&lt;/p&gt;</content><author><name></name></author><category term="scikit-learn" /><category term="phyton" /><category term="sklearn" /><summary type="html">В scikit-learn можно воспользоваться встроенными метриками для отбора модели. Это можно сделать с помощью аргумента scoring, который доступен в том числе и для кросс-валидации и грид-серча. Список доступен в модуле sklearn.metrics.scorer</summary></entry><entry><title type="html">Конвейеры трансформации и кастомные трансформаторы в scikit-learn</title><link href="https://konstantinklepikov.github.io/2019/08/02/sklearn-transformators.html" rel="alternate" type="text/html" title="Конвейеры трансформации и кастомные трансформаторы в scikit-learn" /><published>2019-08-02T00:00:00+02:00</published><updated>2019-08-02T00:00:00+02:00</updated><id>https://konstantinklepikov.github.io/2019/08/02/sklearn-transformators</id><content type="html" xml:base="https://konstantinklepikov.github.io/2019/08/02/sklearn-transformators.html">&lt;p&gt;В scikit-learn есть специальный класс Pipeline, с помощью которого можно создавать конструктор определяющий последовательность из шагов, трансформирующих данные в нужном порядке.&lt;/p&gt;

&lt;p&gt;Выглядит это примерно так:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.pipeline&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;transform_pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'std_scaler'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()),&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pd_to_np'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FunctionTransformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PdtoNp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Вызов метода fit() запускает всю цепочку и приводит к последовательному выполнению трансформаций.&lt;/p&gt;

&lt;p&gt;В данном случае в конвейер включены два шага трансформации данных. Первый - базовый scikit-learn-овский метод стандартизации данных (&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html&quot;&gt;подробнее&lt;/a&gt;). А второй — это кастомный трансформатор.&lt;/p&gt;

&lt;p&gt;Самый простой способ создать собственный трансформатор - это импортировать FunctionTransformer из sklearn.preprocessing. FunctionTransformer отправляет X и опционально y (массив данных и массив меток), а так же пользовательские аргументы в указанную функцию и возвращает результат. В примитивном примере я трансформирую произвольные данные в numpy массив.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FunctionTransformer&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;PdtoNp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;some_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;some_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Подробнее можно прочитать &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html&quot;&gt;базе знаний scikit-learn&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Чтобы создать более сложную конструкцию, можно реализовать собственный класс, а в нем три базовых метода scikit-learn — fit() (должен возвращать self), transform() (собственно сам трансформер) и fit_transform(). Для всех трех методов обязательным аргументом является X. Если унаследовать класс от &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html&quot;&gt;TransformerMixin&lt;/a&gt;, то fit_transformer() можно не задавать, а если добавить в качестве базового класса &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html&quot;&gt;BaseEstimator&lt;/a&gt; и не реализовывать *args, **kwargs в конструкторе, то будут доступны методы get_params() и set_params()&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.base&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BaseEstimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TransformerMixin&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;That&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BaseEstimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TransformerMixin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;That&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Ну и, наконец, можно собирать несколько конвейеров трансформации в один, например, так:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;transform_pipeline1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'std_scaler'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()),&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;transform_pipeline2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pd_to_np'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FunctionTransformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PdtoNp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;full_transform_pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'transform_pipeline1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform_pipeline1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'transform_pipeline2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform_pipeline1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Весь этот набор инструментов scikit-learn превращает обработку данных в довольно креативный и удобный для пользователя процесс.&lt;/p&gt;</content><author><name></name></author><category term="scikit-learn" /><category term="phyton" /><category term="sklearn" /><summary type="html">В scikit-learn есть специальный класс Pipeline, с помощью которого можно создавать конструктор определяющий последовательность из шагов, трансформирующих данные в нужном порядке.</summary></entry></feed>