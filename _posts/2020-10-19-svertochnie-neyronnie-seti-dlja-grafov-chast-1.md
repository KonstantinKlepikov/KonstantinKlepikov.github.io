---
layout: post
title: "Сверточные нейронные сети для графов. Часть 1"
date: 2020-10-19
tags: neural-network graphs convolutional cs224w GCN
tag-for-sollecting: GCN
keywords: Сверточная нейронная сеть граф graph convolutional neural network GCN
image: /assets/img/191020banner.png
---

Графовые нейронные сети --- молодое и перспективное направление развития нейронных сетей, нашедшее применение в анализе различных структур данных, например, социальных профилей, групп документов, в молекулярной биологии, трехмерных изображений и т.д. и т.п. В данной статье разбираются подходы к решению проблемы работы с графовыми данными.

Проблемой обучения моделей на данных, представленных в виде графов, является их плохая структурированность. Информация о нодах и связях между ними описывается, как правило, в многомерном неевклидовом пространстве, что делает векторизацию таких данных затруднительной.

Одним из основных подходов в решении данной задачи является representation learning. Подход сводится к созданию эмбеддингов для нод или субграфов в графе, что позволяет перевести данные в область с низкой размерностью. Такие эмбеддинги уже можно использовать на входе различных моделей ML.

Если мы имеем граф $$G = (V, E)$$ и ассоциированные с ним матрицу смежности $$A$$ и матрицу $$X$$, содержащую атрибуты нод, так что $$X \in \mathbb{R}^{m \times \vert V \vert}$$, то задачей является получение векторов $$z \in \mathbb{R}^d$$ для каждой ноды, таких, что $$d \ll \vert V \vert$$. (Данный подход справедлив и для ребер).

Эту задачу решает такая модель:

- функция похожести $$S_g : V \times V \rightarrow \mathbb{R}^+$$, определенная над графом $$G$$. Функция измеряет схожесть между нодами.
- энкодер, генерирующий эмбеддинги
- декодер, реконструирующий взаимную схожесть нод
- функция потерь

Самый простой метод --- shallow encoding, для которого мы можем определить энкодер как функцию: $$ENC(v_i) = Z v_i$$, где $$Z \in \mathbb{R}^{m \times \vert V \vert}$$.

К подобным методам относятся различные матричные факторизации (graph factorisation, GraRep, HOPE и т.д.) и RandomWalk модели (DeepWalk, node2wec).

Проблемой shallow encoding является то, что в этом методе параметры нод не распространяются в энкодере. Каждый вектор обучается отдельно от других, что ведет к значительному росту сложности. Кроме того, подход не учитывает атрибуты нод, которые в большинстве случаев содержат важную информацию о графе. К тому же подход генерирует эмбеддинги только для данных, которые есть среди обучаемых. Модель проблематично обобщить на данные, которые модель никогда не видела.

Чтобы справиться с недостатками shallow encoding, были изобретены генерализованные энкодер-декодер структуры: DNGR (deep neural graph representation) и SDNE (structural deep network embeddings), относящихся к автоэнкодерам, основанных на исследовании соседних нод в графе.

Прямым решением для таких моделей является то, что каждая нода $$v_i$$ ассоциируется с высокаразмерным <<вектором близости>> $$S_i \in \mathbb{R}^{\vert V \vert}$$, содержащем информацию о близости ноды ко всем другим нодам в графе. Затем данный вектор сжимается до нужной размерности. Проблема такого подхода в том, что он невероятно дорогой. К тому же метод статичный и плохо работает с изменяющимися графами.

Другой подход --- neighbourhood agregation атрибутов нод для генерации эмбеддингов. Этот метод позволяет агрегировать <<месседжи>> от соседей ноды, которые, в свою очередь, базируются на <<месседжах>>, агрегированных по соседям соседей и т.д. Иногда такие модели называют сверточными энкодерами из-за схожести их архитектуры со свертками.

![neighbourhood agregation](../../../assets/img/191020-01.png)

Алгоритм такой сети выглядит так (взято из статьи Representation Learning on Graphs: Methods and Applicationsб Hamilton, William L.; Ying, Rex; Leskovec, Jure);

![neighbourhood agregation algorythm](../../../assets/img/191020-02.png)

Алгоритм строит эмбеддинги для нод рекурсивно. Вначале модель инициализируется входными данными, эквивалентными атрибутам нод. Затем на каждой итерации алгоритма под управлением некой дифференцирующей агрегирующей функции агрегируются эмбеддинги соседей. Затем каждая нода получает новый эмбеддинг, скомбинированный из ее собственного эмбеддинга и агрегированного вектора. Затем все это отправляется в полносвязный слой и затем процесс повторяется $$K$$раз.

К моделям, которые работают по такому принципу относятся многочисленное семейство GCN (graph convolutional networks), column networks и GraphSAGE.

В алгоритме обучаются агрегирующие функции и сет матриц весов $$\{W^K, \forall \mathbb{R} \in [ 1, K ]\}$$, которые и определяют как именно агрегируется информация из области локальной близости ноды. Эти параметры распространяются соседним нодам.

Разные подтипы модели определяются разными агрегирующей функцией (стр.4 алгоритма) и комбинирующей вектора функцией (стр.5). В GCN и columns encoding используется взвешенная сумма в качестве комбинирующей функции и поэлементное среднее для агрегирующей. В GraphSAGE используется конкатенация в качестве комбинирующей функции, а в качестве агрегирующей может использоваться другая нейронная сеть, к примеру, LSTM.
