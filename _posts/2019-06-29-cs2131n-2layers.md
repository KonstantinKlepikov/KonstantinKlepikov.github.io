---
layout: post
title: "CS231n: Two-Layer Neural Network"
date: 2019-06-29
tags: cs231n two-Layer neural-network
tag-for-sollecting: cs231n
keywords: cs231n компьютерное зрение visual recognition нейронные сети deep learning глубокое обучение машинное обучение machine learning data science Two-Layer Neural Network
published: false
---

Итак, в четвертой задаче [Assignment #1: Image Classification, kNN, SVM, Softmax, Neural Network](http://cs231n.github.io/assignments2019/assignment1/ "Assignment #1") необходимо построить классификатор на основе двухслойнной нейронной сети. И хотя задача оценена всего в 25 пунктов и 100, за ней расположен довольно объемный материал, который придется освоить, прежде чем писать решение.

Задание описано в two_layer_net.ipynb. Необходимо прочитать следующие статьи курса: [Backpropagation](http://cs231n.github.io/optimization-2/), [Setting up the Architecture of Neural Networks](http://cs231n.github.io/neural-networks-1/), [About setting Data and the Loss](http://cs231n.github.io/neural-networks-2/), [Learning and Evaluation of Neural Networks](http://cs231n.github.io/neural-networks-3/) и [Emplementation](http://cs231n.github.io/neural-networks-case-study/). Весь этот материал имеет фундаментальное значение для понимания работы нейронных сетей, поэтому желательно в нем детально разобраться.

Напоминаю, что я проходил cs231n на [задачах 2018-го года](https://github.com/cs231n/cs231n.github.io/tree/master/assignments/2018 "cs321s задачи 2018-го года"). Иллюстрации к данной статье взяты из бекграунда курса.

## Что можно узнать, решив задачу?

1. понять принципы построения нейронных сетей

2. разобраться с обратным распространением

3. научиться строить управлять процессом обучения сети

## Backpropagation

Что нам нужно знать об обратном распространении? В задании на построение [svm-классификатора]({{site.baseurl}}{% link _posts/2019-05-31-cs2131n-svm.md %}) разбиралось понятие градиента и градиентного спуска. Рассчет градиента функции потерь ускоряет поиск весов $$W$$, при которых функция потерь становится минимальной. В SVM и softmax мы обновляли веса следующим способом: weights += - step_size * weights_grad, где step_size - это скорость обучения, а weights_grad - градиент. В нейросетях мы пойдем дальше.

/

На практике $$C$$ подбирают таким, чтобы $$\log{C} = - \max_{j} f_{j}$$ равнялся максимальному значению $$f$$. В этом случае это значение станет равным 0, а все остальные будут сдвинуты на величину максимума. Пример из курса

````python
# плохой вариант, проблемы с экспонентой
f = np.array([123, 456, 789])
p = np.exp(f) / np.sum(np.exp(f))

# вычитаем максимальное значение и решаем проблему больших чисел
f -= np.max(f) # [-666, -333, 0]
p = np.exp(f) / np.sum(np.exp(f))
````

Ну и, наконец, пример softmax в сравнении с SVM:

![Softmax vs SVM](../../../assets/img/180619-05.jpg)

В обоих случаях вычисляем один и тот же вектор $$f$$. Разница в функции потерь. SVM подсчитывает значения для корректных меток, которые больше некорректных более чем на заданную постоянную. Softmax же интерпретирует значения как вероятности, переводит их в логарифмический масштаб и нормализует. При этом суммарные значения функции потерь SVM и Softmax классификаторов сравнивать нельзя, так как эти значения имеют смысл только в контексте применяемой функции потерь. Softmax позволяет подсчитать вероятность верной классификации для всех меток, а SVM поставляет некие значения для всех классов, которые трудно интерпретировать. За интерпретацию в softmax приходится платить — распределение вероятностей будет сильно зависеть от силы регуляризации $$\lambda$$.

На практике SVM и Softmax дают примерно одинаковый результат и не сильно отличаются по производительности. Хотите больше подробностей? [Linear classification: Support Vector Machine, Softmax](http://cs231n.github.io/linear-classify/ "Linear classification: Support Vector Machine, Softmax")

## Задачи

Задачи абсолютно аналогичны задачам [svm-classificator]({{site.baseurl}}{% link _posts/2019-05-31-cs2131n-svm.md %}). Необходимо:

- реализовать векторизованную функцию потерь
- посчитать аналитический градиент
- сравнить результат с числовым градиентом
- оптимизировать скорость обучения и силу регуляризации

В softmax.ipynb уже стандартно (мы это делали в двух предыдущих задачах) получаем объекты из дата-сета CIFAR-10, формируем обучающую, валидационную и тестовые выборки, нормализуем данные, вычитая из всех изображений среднее изображение и проворачиваем трюк с баесовским вектором, вынося его в массив весов и соответственно добавляя единицы в $$x_{i}$$.

Затем в softmax.py необходимо реализовать «наивный» softmax-классификатор. Как мы уже знаем, классификатор получает на вход абсолютно все то же самое, что и svm и от svm отличается только функцией потерь. Поэтому смело берем SVM из предыдущей задачи и меняем функцию в теле цикла на softmax-функцию.

````python
def softmax_loss_naive(W, X, y, reg):
  loss = 0.0
  dW = np.zeros_like(W)

  #############################################################################
  # TODO: Compute the softmax loss and its gradient using explicit loops.     #
  # Store the loss in loss and the gradient in dW. If you are not careful     #
  # here, it is easy to run into numeric instability. Don't forget the        #
  # regularization!                                                           #
  #############################################################################
  num_classes = W.shape[1]
  num_train = X.shape[0]

  for i in range(num_train):
    scores = X[i].dot(W)
    correct_class_score = scores[y[i]]

    for j in range(num_classes): #1

    #2 

  loss /= num_train #4
  loss += reg * np.sum(W * W) #4

  #############################################################################
  #                          END OF YOUR CODE                                 #
  #############################################################################

  return loss, dW
````

Во внутреннем цикле (1) считаем знаменатель softmax-функции. Далее, надо посчитать нормализованную softmax-функцию и саму функцию потерь (3), не забыв про регуляризацию.

Следующий шаг - «наивный» градиент.

````python
def softmax_loss_naive(W, X, y, reg):
  loss = 0.0
  dW = np.zeros_like(W)

  num_classes = W.shape[1]
  num_train = X.shape[0]

  for i in range(num_train):
    scores = X[i].dot(W)
    correct_class_score = scores[y[i]]

    for j in range(num_classes): #1

    for j in range(num_classes): #2
    
  dW /= num_train #3
  dW += W * reg

  return loss, dW
````

В той же функции считаем градиент для всего массива (2), а затем сам градиент у же известным нам по svm методом (3). Дальше смотрим на результат в блокноте и сравниваем с численным градиентом. Авторы курса предлагают сравнить результат для разных значений регуляризации. Сравниваем.

Дальше, как и в задаче с svm, необходимо задать softmax_loss_vectorized. По традиции мы эту функцию пишем с нуля.

````python
def softmax_loss_vectorized(W, X, y, reg):
  loss = 0.0
  dW = np.zeros(W.shape)
  num_classes = W.shape[1]
  num_train = X.shape[0]
  scores = X.dot(W) #1
  correct_class_scores = scores[range(num_train), y].reshape((num_train, 1))

  return loss, dW
````

Итак, нужно как-то избавиться от циклов. Для начала пересчитаем scores и correct_class_scores средствами numpy, не прибегая к циклам. Отсюда логика всех дальнейших действий становится предельно ясной - так же считаем softmax-функцию, функцию потерь и градиент, используя встроенные методы numpy, такие как np.sum. Решение довольно простое, а его результат по производительности покажет выигрыш по сравнению с циклами примерно в 50 раз.

Осталось только поэкспериментировать с гиперпараметрами - силой регуляризации и скоростью обучения. Задача абсолютно идентично той, что решалась в последней части задания на [svm-classificator]({{site.baseurl}}{% link _posts/2019-05-31-cs2131n-svm.md %}). Берем кусок кода, который мы делали для svm, вместо LinearSVM() получаем объект Softmax() и подкручиваем гиперпараметры так, чтобы точность перешагнула через 0.4. Можно посмотреть результат для тех же значений гиперпараметров, что использовались для svm. На этом примере как раз и можно увидеть, как softmax-классификатор зависит от силы регуляризации.

На этом практическая часть курса, связанная с линейными классификаторами, завершается. Следующая задача - построение простейшей двухслойной нейронной сети.
